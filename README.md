
AI-Based CV Screening Prototype

Overview

This project is a simple prototype of an AI-assisted CV screening system.
The goal of the system is to automate the initial evaluation of job candidates by analyzing their CVs and project reports against predefined company standards.

This prototype focuses on system flow, AI integration, and error handling, rather than building a full production backend.

System Workflow

The system receives a candidate’s CV and project report as text input.

Inputs are validated before processing.

The evaluation process runs in the background using an AI evaluator.

The CV is evaluated against the job description and CV evaluation rubric.

The project report is evaluated against the case study brief and project rubric.

The evaluation results are stored and can be reviewed later.

The process is designed so that users do not need to wait for the evaluation to complete.

System Architecture

The prototype is intentionally kept simple and is divided into clear responsibilities:

Main Application (main.py)
Handles input validation, execution flow, and result handling.

AI Evaluation Module (ai_evaluator.py)
Responsible for interacting with an external LLM service and performing structured evaluations.

Prompt Definitions (prompts/)
Contains constrained prompts that define the role, scope, and output format of the AI.

Sample Data (sample_data/)
Provides example inputs to make the prototype easy to test and reproduce.

This separation ensures clarity, maintainability, and controlled AI behavior.

AI Evaluation Design

AI is used as an evaluation assistant, not a decision maker.

Key design principles:

AI evaluates candidates strictly based on provided context (job description, case study brief, and rubrics).

AI does not make hiring decisions.

AI outputs are structured and validated before being accepted.

Two separate evaluations are performed:

CV Evaluation — produces a match score and qualitative feedback.

Project Evaluation — produces a score and feedback based on the case study requirements.

A final summary is generated by combining both results.

Prompt Constraints

The AI is constrained in the following ways:

Limited to the provided documents only.

Required to output results in a fixed JSON format.

Restricted to scoring and feedback, not final decisions.

This ensures consistency, reliability, and explainability of results.

Error Handling & Reliability

The system is designed to remain stable even when failures occur.

Handled scenarios include:

Missing or empty input data.

AI API timeouts or failures.

Invalid or malformed AI responses.

Scores outside the expected range.

Error handling strategy:

AI calls are retried a limited number of times.

Invalid outputs are rejected and retried.

If failures persist, the process is marked as failed with a clear error message.

This approach prevents silent failures and makes the system behavior predictable.

LLM Integration

The AI evaluation module is designed to integrate with an external LLM provider (e.g. OpenRouter, OpenAI, or Gemini).

For simplicity and reproducibility:

API credentials are read from environment variables.

If no API key is provided, a mock response is used to demonstrate the system flow.

Limitations

This prototype is intentionally minimal:

No user interface.

No authentication.

No persistent database.

No production deployment setup.

The focus is on demonstrating correct system design and AI usage, not production completeness.

How to Run

Prepare sample input data (or use the provided examples).

Run the main script: python app/main.py 

Review the printed or saved evaluation results.

Conclusion

This prototype demonstrates how AI can be responsibly integrated into a CV screening workflow with clear boundaries, structured outputs, and robust error handling.

